{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e732e4dc",
   "metadata": {},
   "source": [
    "# Naive Bayes text Document Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff56781",
   "metadata": {},
   "source": [
    "<h3>Importing Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff64a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f96fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this block generates class_id, document_id, count of documents per class and a list containing paths to all the \n",
    "#files in the training dataset\n",
    "directory = \"train\"\n",
    "def pre_process(directory):\n",
    "    paths = []              #contains paths to all the files of the dataset\n",
    "    count_perclass = []  #contains count of documents in every class\n",
    "    class_doc = []\n",
    "    Class_id = {}           #assigns an id to each class\n",
    "    doc_id_dict = {}        #assigns an id to each document\n",
    "    id = 1\n",
    "    doc_id = 1\n",
    "    for subdir in os.listdir(directory):\n",
    "        for document in os.listdir(directory+\"/\"+subdir):\n",
    "            doc_id_dict[document] = doc_id\n",
    "            doc_id+= 1\n",
    "            class_doc.append([id , doc_id])\n",
    "        Class_id[subdir] = id\n",
    "        id+=1\n",
    "        files = os.listdir(directory+'/'+subdir)\n",
    "        paths.extend([ directory+'/'+subdir+'/'+i for i in files])\n",
    "        count_perclass.append(len(files))\n",
    "        \n",
    "    class_doc = pd.DataFrame(class_doc) \n",
    "    \n",
    "    return Class_id , doc_id_dict , class_doc , count_perclass , paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9800f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this block generates the total words dictionary and assigns a word ID to each word.\n",
    "def generate_total_words():\n",
    "    total_words = {}        #dictionary containing all the words as keys and their word id as values.\n",
    "    for f in paths:\n",
    "        file_words = []\n",
    "        file = open(f , \"r\")\n",
    "        for line in file.readlines():\n",
    "            for word in line.split(\" \"):\n",
    "                word = word.replace(\"\\n\" , \"\")\n",
    "                word = word.replace(\".\" , \"\")\n",
    "                word = word.replace(\",\" , \"\")\n",
    "                word = word.replace(\". \" , \"\")\n",
    "                word = word.replace(\". \" , \"\")\n",
    "                word = word.replace(\".\\n\" , \"\")\n",
    "                word = word.replace(\",\\n\" , \"\")\n",
    "                word = word.replace(\"'s\" , \"\")\n",
    "                word = word.replace(\"s\" , \"\")\n",
    "                word = LancasterStemmer().stem(word)\n",
    "                total_words[word] = 0\n",
    "\n",
    "    w_id = 1\n",
    "    for word in total_words.keys():\n",
    "        total_words[word] = w_id\n",
    "        w_id += 1\n",
    "        \n",
    "    return total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this block generates a data list which contains information about every word in the dataset\n",
    "def generate_data_df(directory , Class_id , doc_id_dict , total_words):\n",
    "    data = []     #a list containing lists of all the words i.e [Class_id , document_id , total_words , word_frequency]\n",
    "    for subdir in os.listdir(directory):\n",
    "        pseudo_data = []\n",
    "        #iterating over every document in every class\n",
    "        for document in os.listdir(directory+\"/\"+subdir):\n",
    "            word_frequency = {}\n",
    "\n",
    "            file = open(directory + \"/\" + subdir + \"/\" + document , \"r\")\n",
    "\n",
    "            #iterating over every line of every document in a class\n",
    "            for line in file.readlines():\n",
    "                #iterating over eery word of the line in a document\n",
    "                for word in line.split(\" \"):\n",
    "                    word = word.replace(\"\\n\" , \"\")\n",
    "                    word = word.replace(\".\" , \"\")\n",
    "                    word = word.replace(\",\" , \"\")\n",
    "                    word = word.replace(\". \" , \"\")\n",
    "                    word = word.replace(\". \" , \"\")\n",
    "                    word = word.replace(\".\\n\" , \"\")\n",
    "                    word = word.replace(\",\\n\" , \"\")\n",
    "                    word = word.replace(\"'s\" , \"\")\n",
    "                    word = word.replace(\"s\" , \"\")\n",
    "                    word = LancasterStemmer().stem(word)\n",
    "\n",
    "                    if word in word_frequency:\n",
    "                        word_frequency[word] += 1\n",
    "                    else:\n",
    "                        word_frequency[word] = 0\n",
    "\n",
    "            for word, count in word_frequency.items():\n",
    "                pseudo_data = [Class_id[subdir] , doc_id_dict[document] , total_words[word] , count]\n",
    "                data.append(pseudo_data)\n",
    "                \n",
    "    data_df = pd.DataFrame(data)\n",
    "    data_df.rename(columns = {0:\"Class_id\" , 1:\"document_id\" , 2:\"word_id\" , 3:\"word_frequency\"} , inplace = True)\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b825532",
   "metadata": {},
   "source": [
    "<h3>Pre-processing the training dataset to test against our naive bayes agent</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_id , doc_id_dict , class_doc , count_perclass , paths = pre_process(directory)\n",
    "total_words = generate_total_words()\n",
    "data_df = generate_data_df(directory , Class_id , doc_id_dict , total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8da58",
   "metadata": {},
   "source": [
    "<h4>pre-prosssed dataframe</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f71b9cb",
   "metadata": {},
   "source": [
    "<h3>Generating probability for every class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this block generates the probability of every class in the taining dataset\n",
    "def generate_class_prob(directory):\n",
    "    prob_class = {}         #contains probability of each class in the dataset\n",
    "    for Class in os.listdir(directory):\n",
    "        files = os.listdir(directory+'/'+Class)\n",
    "        prob = len(files) / len(paths)\n",
    "        prob_class[Class_id[Class]] = prob\n",
    "    return prob_class\n",
    "\n",
    "prob_class = generate_class_prob(directory)\n",
    "prob_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d24fb",
   "metadata": {},
   "source": [
    "<h3>Applying laplace smoothing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9316d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with laplace smoothing\n",
    "a = 1\n",
    "\n",
    "#probability of each word given class\n",
    "pb_ij = data_df.groupby(['Class_id','word_id'])\n",
    "pb_j = data_df.groupby(['Class_id'])\n",
    "Pr_a =  (pb_ij['word_frequency'].sum() + a) / (pb_j['word_frequency'].sum() + len(total_words))\n",
    "\n",
    "Pr_a = Pr_a.unstack()\n",
    "\n",
    "for c in range(1,11):\n",
    "     Pr_a.loc[c,:] = Pr_a.loc[c,:].fillna(a/(pb_j['word_frequency'].sum()[c] + len(total_words)))\n",
    "\n",
    "Pr_dict_a = Pr_a.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dedb9e3",
   "metadata": {},
   "source": [
    "<h3>Probability without laplace smoothing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without laplace smoothing\n",
    "\n",
    "#probability of each word given class\n",
    "pb_kl = data_df.groupby(['Class_id','word_id'])\n",
    "pb_l = data_df.groupby(['Class_id'])\n",
    "Pr =  (pb_kl['word_frequency'].sum()) / (pb_l['word_frequency'].sum())\n",
    "\n",
    "Pr = Pr.unstack()\n",
    "\n",
    "for c in range(1,11):\n",
    "  Pr.loc[c,:] = Pr.loc[c,:]\n",
    "\n",
    "Pr_dict = Pr.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd496d2",
   "metadata": {},
   "source": [
    "<h3>Removing Stopwords</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e02884",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words_df = pd.DataFrame(list(total_words.items()))\n",
    "\n",
    "#word_id of all words\n",
    "total = set(total_words_df[1])\n",
    "\n",
    "#generating set of good words\n",
    "total_words_df = total_words_df[~total_words_df[0].isin(stop_words)]\n",
    "good = list(total_words_df[1])\n",
    "good = set(good)\n",
    "\n",
    "#generating set of stop words\n",
    "stop = total - good\n",
    "\n",
    "for bad in stop:\n",
    "    for j in range(1,11):\n",
    "        Pr_dict[j][bad] = a/(pb_j['word_frequency'].sum()[j] + len(total_words))    #removing stop words\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680f4c6",
   "metadata": {},
   "source": [
    "<h3>Calculating probability of every class given document of test dataset and then assigning class to every document</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_class(Pr_dict , dict):\n",
    "    result = []\n",
    "    for doc_id in range(1, len(dict)+1):\n",
    "        prob_dict = {}\n",
    "        for class_id in range(1,len(count_perclass) + 1):\n",
    "            prob_dict[class_id] = 1\n",
    "            for word_id in dict[doc_id]: \n",
    "                prob=Pr_dict[word_id][class_id]\n",
    "                if prob != 0:            \n",
    "                    prob_dict[class_id]+=(np.log(1+ dict[doc_id][word_id]))*np.log(prob)\n",
    "                else:\n",
    "                    prob_dict[class_id] += 0       \n",
    "#calculating final probability of each word      \n",
    "            prob_dict[class_id] +=  np.log(prob_class[class_id])                          \n",
    "            \n",
    "#finding maximum value of probability\n",
    "        max_probability = max(prob_dict, key=prob_dict.get)\n",
    "        result.append(max_probability)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124cb6f",
   "metadata": {},
   "source": [
    "<h2>Creating a new dictionary to store the frequency, doc_id and word_count and to provide a way to see the results with and without laplace smoothing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d0eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm(df , laplace_smoothing= False):\n",
    "\n",
    "    df_dict = df.to_dict()\n",
    "#new_dict is a dictionary containing keys as document id's and values as dictionaries containing wordId as keys\n",
    "#and word frequency as values.\n",
    "    new_dict = {}\n",
    "    \n",
    "    for ID in range(len(df_dict['document_id'])):\n",
    "        doc_id = df_dict['document_id'][ID]\n",
    "        word_id = df_dict['word_id'][ID]\n",
    "        frequency = df_dict['word_frequency'][ID]\n",
    "        try: \n",
    "            new_dict[doc_id][word_id] = df_dict['word_frequency'][ID] \n",
    "        except:\n",
    "            new_dict[df_dict['document_id'][ID]] = {}\n",
    "            new_dict[doc_id][word_id] = df_dict['word_frequency'][ID]\n",
    "  \n",
    "    return assign_class(Pr_dict_a , new_dict) if laplace_smoothing else assign_class(Pr_dict , new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cbe3b9",
   "metadata": {},
   "source": [
    "<h3>Testing our model against the training dataset and calculating the error</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d856c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = data_df\n",
    "\n",
    "classification_withLaplace = algorithm(test_data_df , True)\n",
    "classification_withoutLaplace = algorithm(test_data_df , False)\n",
    "\n",
    "#training dataframe for verification of our model\n",
    "\n",
    "training_data = list(class_doc[0]) \n",
    "correctness = 0\n",
    "correctness_a = 0\n",
    "\n",
    "for x,y in zip(classification_withoutLaplace, training_data):\n",
    "    if x != y:\n",
    "        correctness +=1\n",
    "    else:\n",
    "        pass   \n",
    "print(\"Error without laplace:\\t\\t\", \"{0:.5f}\".format(correctness/ len(training_data) *100), \"%\")\n",
    "\n",
    "for x,y in zip(classification_withLaplace, training_data):\n",
    "    if x != y:\n",
    "        correctness_a +=1\n",
    "    else:\n",
    "        pass \n",
    "print(\"Error with laplace:\\t\\t\", \"{0:.5f}\".format(correctness_a/ len(training_data) *100) , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5fc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fd5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
